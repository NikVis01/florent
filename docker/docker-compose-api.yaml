services:
  florent-api:
    build:
      context: ..
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - LLM_MODEL=${LLM_MODEL:-gpt-4-turbo-preview}
      - BGE_M3_URL=${BGE_M3_URL:-http://bge-m3:8080}
    depends_on:
      bge-m3:
        condition: service_healthy
    restart: unless-stopped
    volumes:
      - ../src:/app/src:ro
      - ../src/data:/app/src/data:ro
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M

  bge-m3:
    image: semitechnologies/transformers-inference:baai-bge-m3-onnx-latest
    ports:
      - "8080:8080"
    restart: unless-stopped
    entrypoint: ["/bin/sh", "-c"]
    command: ["apt-get update -qq && apt-get install -y -qq curl >/dev/null 2>&1; exec /bin/ollama serve"]
    healthcheck:
      test: [ "CMD", "python3", "-c", "import socket; s = socket.socket(); s.connect(('localhost', 8080))" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '1.0'
          memory: 2G
